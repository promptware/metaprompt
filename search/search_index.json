{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Module system Syntax","title":"Home"},{"location":"modules/","text":"MetaPrompt module system is centered around files.","title":"Modules"},{"location":"syntax/","text":"Text \u00b6 A prompt is usually a valid metaprompt: Hi, LLM! How are you feeling today? will be expanded to the same string, because it does not contain any MetaPrompt constructs. Variables \u00b6 Here's a variable: [:variable_name]. If a variable is used before first assignment, it is treated as a required prompt parameter automatically. [:variable_name=it can be reassigned to any value, however. Including a value containing its old value: [:variable_name] or referencing [:other] variables] Comments \u00b6 [# Text for the human reader can be written like this. Comments must be well-formed metaprompt expressions too - in the future comment parse trees will be used to convey additional semantic info (e.g. documentation). Comments are ignored during evaluation. ] Conditionals \u00b6 [:if the sky is sometimes blue :then this... :else that... ] [# ^ This expression will be expanded at runtime. First, the following text will be fed to an LLM: Please determine if the following statement is true. Do not write any other output, answer just \"true\" or \"false\". The statement: the sky is sometimes blue The answer will determine the execution branch. If the answer is not literally \"true\" or \"false\", an exception will be thrown after a few retries ] Meta-prompting \u00b6 LLM says: [$ Hi, LLM! How are you today?] [# ^^^ this prompt will be executed and its output will be inserted at its position during expansion ] A more inspiring example: [$ Improve this LLM prompt: [:prompt]] Modules \u00b6 [:use ./relative-import] [:use package-name/directory/module] With parameters: [:use ./relative-import :someParameter= arbitrary value, potentially using any other MetaPrompt constructs :otherParameter= another value ]","title":"Text"},{"location":"syntax/#text","text":"A prompt is usually a valid metaprompt: Hi, LLM! How are you feeling today? will be expanded to the same string, because it does not contain any MetaPrompt constructs.","title":"Text"},{"location":"syntax/#variables","text":"Here's a variable: [:variable_name]. If a variable is used before first assignment, it is treated as a required prompt parameter automatically. [:variable_name=it can be reassigned to any value, however. Including a value containing its old value: [:variable_name] or referencing [:other] variables]","title":"Variables"},{"location":"syntax/#comments","text":"[# Text for the human reader can be written like this. Comments must be well-formed metaprompt expressions too - in the future comment parse trees will be used to convey additional semantic info (e.g. documentation). Comments are ignored during evaluation. ]","title":"Comments"},{"location":"syntax/#conditionals","text":"[:if the sky is sometimes blue :then this... :else that... ] [# ^ This expression will be expanded at runtime. First, the following text will be fed to an LLM: Please determine if the following statement is true. Do not write any other output, answer just \"true\" or \"false\". The statement: the sky is sometimes blue The answer will determine the execution branch. If the answer is not literally \"true\" or \"false\", an exception will be thrown after a few retries ]","title":"Conditionals"},{"location":"syntax/#meta-prompting","text":"LLM says: [$ Hi, LLM! How are you today?] [# ^^^ this prompt will be executed and its output will be inserted at its position during expansion ] A more inspiring example: [$ Improve this LLM prompt: [:prompt]]","title":"Meta-prompting"},{"location":"syntax/#modules","text":"[:use ./relative-import] [:use package-name/directory/module] With parameters: [:use ./relative-import :someParameter= arbitrary value, potentially using any other MetaPrompt constructs :otherParameter= another value ]","title":"Modules"},{"location":"tutorial/","text":"Overview \u00b6 Metaprompt is a language for prompt automation, structuring and reuse. One one side, it is very similar to a template engine like Jinja or EJS. The twist is, metaprompt expansion depends on LLM outputs of LLM queries, that are formed in natural language. Use cases \u00b6 Prompt organization \u00b6 A module system and a package system allow anyone to assign identities to promps and package them as callable functions. Meta-prompting \u00b6 Meta-prompting is a technique of asking an LLM to create an LLM prompt. Using prompts to craft task-specific prompts based on a set of high level principles Modifying prompts to increase accuracy Securing inputs from prompt injection attacks Model selection based on prompt contents Templating \u00b6 The most basic use case of metaprompt is substituting parameter values instead of variable names embedded in a prompt.","title":"Overview"},{"location":"tutorial/#overview","text":"Metaprompt is a language for prompt automation, structuring and reuse. One one side, it is very similar to a template engine like Jinja or EJS. The twist is, metaprompt expansion depends on LLM outputs of LLM queries, that are formed in natural language.","title":"Overview"},{"location":"tutorial/#use-cases","text":"","title":"Use cases"},{"location":"tutorial/#prompt-organization","text":"A module system and a package system allow anyone to assign identities to promps and package them as callable functions.","title":"Prompt organization"},{"location":"tutorial/#meta-prompting","text":"Meta-prompting is a technique of asking an LLM to create an LLM prompt. Using prompts to craft task-specific prompts based on a set of high level principles Modifying prompts to increase accuracy Securing inputs from prompt injection attacks Model selection based on prompt contents","title":"Meta-prompting"},{"location":"tutorial/#templating","text":"The most basic use case of metaprompt is substituting parameter values instead of variable names embedded in a prompt.","title":"Templating"}]}